---
title: "SwEdemo Notes"
author: "T Nichols"
date: "24/03/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This provides the math-mode exposition of the computational strategy used in the `SwEdemo.R` R script. 

## Set up

For data $Y$ with $N$ observations and design matrix $X$ with $P$ predictors, let the OLS estimator of the GLM
$$
Y=X\beta+\epsilon
$$
be 
$$
\hat\beta_{\mathrm{OLS}} = (X^\top X)^{-1}X^\top Y,
$$
and let the OLS residuals be
$$
e = Y-X\hat\beta_{\mathrm{OLS}}.
$$

We allow for dependence among $B$ groups or clusters of observations. Specifically let the model be partitioned row-wise such that
$$
Y^\top = (Y_1,Y_2,...,Y_i,...,Y_{B})^\top
$$
$$
X^\top = (X_1,X_2,...,X_i,...,X_{B})^\top
$$
$$
\epsilon^\top = (\epsilon_1,\epsilon_2,...,\epsilon_i,...,\epsilon_{B})^\top,
$$
where $Var(\epsilon_{i})=V_i$, and $\epsilon_{i}$ and $\epsilon_{i'}$ are mutually independent for any $i\neq i'$.

## Robust / Sandwich estimator of variance

The following estimator of $Var(\hat\beta_{\mathrm{OLS}})$ is consistent as $B$ grows:

$$
S = (X^\top X)^{-1} \left(\sum_{i=1}^B X_i^\top \hat{V}_i X_i\right) (X^\top X)^{-1}
$$
where the variance for block $i$ is estimated naively
$$
e_i = e_i e_i^\top.
$$
In particular, note that there are no assumptions about the form of each $V_i$... there is no assumption about constant variance or homogeneous intra-block correlation.

## Reformulation for efficient computation
 
To accelerate the computation we must obtain formulations that accommodate $Y$ having the structure of a large matrix with $N_{\mathrm{elm}}$ columns, such that $\hat\beta$ and $e$ will also have  $N_{\mathrm{elm}}$ columns.

OLS estimation trivially generalizes, as $(X^\top X)^{-1}X^\top$ operates as a left-multiplication of an matrix with $N$ rows and an arbitrary number of columns.

The variance estimator can also be reformulated

$$
\begin{eqnarray}
 S &=&  (X^\top X)^{-1} \left(\sum_{i=1}^B X_i^\top \hat{V}_i X_i\right) (X^\top X)^{-1}\\
   &=& \sum_{i=1}^B (X^\top X)^{-1}  X_i^\top \hat{V}_i X_i (X^\top X)^{-1}\\
   &=& \sum_{i=1}^B (X^\top X)^{-1}  X_i^\top e_i e_i^\top X_i (X^\top X)^{-1}\\
   &=& \sum_{i=1}^B E_i E_i^\top,
\end{eqnarray}
$$
where
$$
E_i = (X^\top X)^{-1}X_i^\top e_i
$$
can be configured as a $P\times 1\times N_{\mathrm{elm}}$ multi-way array, and then an outer product of the first two dimensions will lead to a computation of $S$ as a $P\times P \times  N_{\mathrm{elm}}$ array.


## Global Working Covariance

Using a voxel-specific working covariance precludes the use of easy vectorisation. However, if the working covariance is global, then the method is still suitable for vectorisation.

The usual way this estimate is written is 
$$
\hat\beta_{\mathrm{G}} = \left(\sum_i X_i^\top W_{\mathrm{G}i} X_i\right)^{-1}\sum_iX_i^\top W_{\mathrm{G}i}Y_i,
$$
where $W_{\mathrm{G}i}=V_{\mathrm{G}i}^{-1}$ and $V_{\mathrm{G}i}$ is the global working covariance for block $i$.  

However, it is probably faster to create an $P\times N$ estimation matrix
$$
\hat\beta_{\mathrm{G}} = \left((X^\top W_{\mathrm{G}} X)^{-1}X^\top W_{\mathrm{G}}\right)Y,
$$
and apply to all data at once, where $W_{\mathrm{G}}$ is the block diagonal matrix with blocks $W_{\mathrm{G}i}$.

For the sandwich estimator computation we see 
$$
\begin{eqnarray}
 S &=&  (X^\top W_{\mathrm{G}}X)^{-1} \left(\sum_{i=1}^B X_i^\top W_{\mathrm{G}i} \hat{V}_i W_{\mathrm{G}i}X_i\right) (X^\top W_{\mathrm{G}}X)^{-1}\\
  &=&  \sum_{i=1}^B (X^\top W_{\mathrm{G}}X)^{-1} X_i^\top W_{\mathrm{Gi}} \hat{V}_i W_{\mathrm{G}i}X_i (X^\top W_{\mathrm{G}}X)^{-1}\\
  &=&  \sum_{i=1}^B (X^\top W_{\mathrm{G}}X)^{-1} X_i^\top W_{\mathrm{G}i} e_i e_i^\top W_{\mathrm{G}i}X_i (X^\top W_{\mathrm{G}}X)^{-1}\\
   &=& \sum_{i=1}^B E_{\mathrm{W}i} E_{\mathrm{W}i}^\top,
\end{eqnarray}
$$
where
$$
E_{\mathrm{W}i} =  (X^\top W_{\mathrm{G}}X)^{-1} X_i^\top W_{\mathrm{G}i}e_i
$$
can again be configured as a 3-way array with an outer product computed on the first two dimensions.

# Singleton contribution

Note that when block $i$ corresponds to but a single observation, we have
$$
E_{\mathrm{W}i} E_{\mathrm{W}i}^\top =  (X^\top W_{\mathrm{G}}X)^{-1} X_i^\top X_i  (X^\top W_{\mathrm{G}}X)^{-1}\; W_{Gi}^2e_i^2.
$$
If data are particularlly poorly behaved this might corrupt the estimate.  This could be addressed by pooling all such contributions to $S$ by assuming a common variance over all (or blocks) of singleton subjects.
