---
title: "SwEdemo Notes"
author: "T Nichols"
date: "24/03/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This provides the math-mode exposition of the computational strategy used in the `SwEdemo.R` R script. 

## Set up

For data $Y$ with $N$ observations and design matrix $X$ with $P$ predictors, let the OLS estimator of the GLM
$$
Y=X\beta+\epsilon
$$
be 
$$
\hat\beta_{\mathrm{OLS}} = (X^\top X)^{-1}X^\top Y,
$$
and let the OLS residuals be
$$
e = Y-X\hat\beta_{\mathrm{OLS}}.
$$

We allow for dependence among $B$ groups or clusters of observations. Specifically let the model be partitioned row-wise such that
$$
Y^\top = (Y_1,Y_2,...,Y_i,...,Y_{B})^\top
$$
$$
X^\top = (X_1,X_2,...,X_i,...,X_{B})^\top
$$
$$
\epsilon^\top = (\epsilon_1,\epsilon_2,...,\epsilon_i,...,\epsilon_{B})^\top,
$$
where $Var(\epsilon_{i})=V_i$, and $\epsilon_{i}$ and $\epsilon_{i'}$ are mutually independent for any $i\neq i'$.

## Robust / Sandwich estimator of variance

The following estimator of $Var(\hat\beta_{\mathrm{OLS}})$ is consistent as $B$ grows:

$$
S = (X^\top X)^{-1} \left(\sum_{i=1}^B X_i^\top \hat{V}_i X_i\right) (X^\top X)^{-1}
$$
where the variance for block $i$ is estimated naively
$$
e_i = e_i e_i^\top.
$$
In particular, note that there are no assumptions about the form of each $V_i$... there is no assumption about constant variance or homogeneous intra-block correlation.

## Reformulation for efficient computation
 
To accelerate the computation we must obtain formulations that accommodate $Y$ having the structure of a large matrix with $N_{\mathrm{elm}}$ columns, such that $\hat\beta$ and $e$ will also have  $N_{\mathrm{elm}}$ columns.

OLS estimation trivially generalizes, as $(X^\top X)^{-1}X^\top$ operates as a left-multiplication of an matrix with $N$ rows and an arbitrary number of columns.

The variance estimator can also be reformulated

$$
\begin{eqnarray}
 S &=&  (X^\top X)^{-1} \left(\sum_{i=1}^B X_i^\top \hat{V}_i X_i\right) (X^\top X)^{-1}\\
   &=& \sum_{i=1}^B (X^\top X)^{-1}  X_i^\top \hat{V}_i X_i (X^\top X)^{-1}\\
   &=& \sum_{i=1}^B (X^\top X)^{-1}  X_i^\top e_i e_i^\top X_i (X^\top X)^{-1}\\
   &=& \sum_{i=1}^B E_i E_i^\top,
\end{eqnarray}
$$
where
$$
E_i = (X^\top X)^{-1}X_i^\top e_i
$$
can be configured as a $P\times 1\times N_{\mathrm{elm}}$ multi-way array, and then an outer product of the first two dimensions will lead to a computation of $S$ as a $P\times P \times  N_{\mathrm{elm}}$ array.


## Global Working Covariance

Using a voxel-specific working covariance precludes the use of easy vectorisation. However, if the working covariance is global, then the method is still suitable for vectorisation.

The usual way this estimate is written is 
$$
\hat\beta_{\mathrm{G}} = \left(\sum_i X_i^\top W_{\mathrm{G}i} X_i\right)^{-1}\sum_iX_i^\top W_{\mathrm{G}i}Y_i,
$$
where $W_{\mathrm{G}i}=V_{\mathrm{G}i}^{-1}$ and $V_{\mathrm{G}i}$ is the global working covariance for block $i$.  

However, it is probably faster to create an $P\times N$ estimation matrix
$$
\hat\beta_{\mathrm{G}} = \left((X^\top W_{\mathrm{G}} X)^{-1}X^\top W_{\mathrm{G}}\right)Y,
$$
and apply to all data at once, where $W_{\mathrm{G}}$ is the block diagonal matrix with blocks $W_{\mathrm{G}i}$.

For the sandwich estimator computation we see 
$$
\begin{eqnarray}
 S &=&  (X^\top W_{\mathrm{G}}X)^{-1} \left(\sum_{i=1}^B X_i^\top W_{\mathrm{G}i} \hat{V}_i W_{\mathrm{G}i}X_i\right) (X^\top W_{\mathrm{G}}X)^{-1}\\
  &=&  \sum_{i=1}^B (X^\top W_{\mathrm{G}}X)^{-1} X_i^\top W_{\mathrm{Gi}} \hat{V}_i W_{\mathrm{G}i}X_i (X^\top W_{\mathrm{G}}X)^{-1}\\
  &=&  \sum_{i=1}^B (X^\top W_{\mathrm{G}}X)^{-1} X_i^\top W_{\mathrm{G}i} e_i e_i^\top W_{\mathrm{G}i}X_i (X^\top W_{\mathrm{G}}X)^{-1}\\
   &=& \sum_{i=1}^B E_{\mathrm{W}i} E_{\mathrm{W}i}^\top,
\end{eqnarray}
$$
where
$$
E_{\mathrm{W}i} =  (X^\top W_{\mathrm{G}}X)^{-1} X_i^\top W_{\mathrm{G}i}e_i
$$
can again be configured as a 3-way array with an outer product computed on the first two dimensions.

# Singleton contribution

Note that when block $i$ corresponds to but a single observation, we have
$$
E_{\mathrm{W}i} E_{\mathrm{W}i}^\top =  (X^\top W_{\mathrm{G}}X)^{-1} X_i^\top X_i  (X^\top W_{\mathrm{G}}X)^{-1}\; W_{Gi}^2e_i^2.
$$
If data are particularly poorly behaved this might corrupt the estimate.  This could be addressed by pooling all such contributions to $S$ by assuming a common variance over all (or blocks) of singleton subjects.

# Residual Adjustments and Hat Matrix with Working Covariance

The residuals $e_i$ are used as plug-in estimates of $\epsilon_i$, but of course the residuals have well known heteroscedasticity and dependence not present in the unobservable errors, i.e. in the usual setting of iid errors, $\mathrm{Var}(\epsilon)=I-H$, where $H=X^\top(X^\top X)^{-1}X^\top$.  
Within the econometrics literature concerned with only with heteroscedasticity errors (not clustered errors), there is an boundless stream of papers evaluating and proposing different residual adjustments that account for residual-forming-induced heteroscedasticity.  
The R package `sandwich`'s vignette "Econometric Computing with HC and HAC Covariance Matrix Estimators" covers most, but Salem et al. (2019) covers even more.  
All of the residual adjustments are based on the (OLS) leverage values
$$
h_{ii} = \mathrm{diag}(H).
$$
Investigations by Bryan Guillaume suggested that all of these corrections work pretty similarly, but is now apparent he wasn't considering the case of highly skewed design matrix that produce highly influential observations, which in turn produce highly deflated residual values.  
In the high-influence design setting, the common obvious correction (known as HC2), where residuals are scaled by $(\mathrm{Var}(\epsilon)_{ii})^{-1/2}=1/\sqrt{1-h_{ii}})$, performs very poorly.  The problem is that this insufficiently accounts for the deflation of the residual from a high-influence design matrix row.  Hence all of the corrections amount to computing higher powers of $1-h_{ii}$ to induce a strong inflation of the problem residuals.

When we are using an identity working covariance matrix, we are using OLS estimates and the hat matrix is well defined.  
However, none of the econometrics literature considers the case when you have a non-identity working covariance $W$.  
(To be fair, for a single univarate dataset there probably aren't any good choices for $W$, but in imaging we can of course pool across voxels to construct a reasonable $W$.)
Hence, now there is ambiguity:  Is the 'hat matrix' the matrix that transforms $Y$ to a prediction of $\hat{Y}$ using the working covariance, i.e. using $\hat{\beta}_{\mathrm{G}}$  That is, is it
$$
\hat{Y}_{\mathrm{G}} = X\hat{\beta}_{\mathrm{G}}=X\left((X^\top W_{\mathrm{G}} X)^{-1}X^\top W_{\mathrm{G}}\right)Y
$$
which implies an asymmetric (but still idempotent) hat matrix

$$
H = X(X^\top W_{\mathrm{G}} X)^{-1}X^\top W_{\mathrm{G}}?
$$
It turns out that the proof of $0 \leq h_{ii} \leq 1$ depends on $H$ being symmetric, and empirically you see $h_{ii}<0$ when $X$ has skewed (e.g. rare binary) predictors if you use this hat matrix.

Alternatively, you can work in the whitened domain, where data and design have been pre-multiplied by $W^{1/2}$.  You can then regard the hat matrix to be that which transforms whitened data $Y^*_{\mathrm{G}}=W^{1/2}_{\mathrm{G}}Y$ into a whitened prediction $\hat{Y}^*_{\mathrm{G}}$: $$
\hat{Y}^*_{\mathrm{G}} = W^{1/2}_{\mathrm{G}}X\hat{\beta}_G = W^{1/2}_{\mathrm{G}}X(X^\top W_{\mathrm{G}} X)^{-1}X^\top W_{\mathrm{G}}^{1/2} Y^*.
$$
In this case we again have a symmetric hat matrix
$$
H_{\mathrm{G}} = W^{1/2}_{\mathrm{G}}X(X^\top W_{\mathrm{G}} X)^{-1}X^\top W^{1/2}_{\mathrm{G}}.
$$

I have opted for $H_{\mathrm{G}}$, since it avoids insanity of negative $h_{ii}$, plus it doesn't alter how you compute the residuals... i.e. since the whitened residuals $e^*$ are in fact just the residuals whitened:
$$
\begin{eqnarray}
e^* &=& Y^*_{\mathrm{G}}-\hat{Y}^*_{\mathrm{G}}\\
 &=& W^{1/2}_{\mathrm{G}}Y -  W^{1/2}_{\mathrm{G}}X\hat{\beta}_G\\
 &=&  W^{1/2}_{\mathrm{G}}\left(Y -  X\hat{\beta}_{\mathrm{G}}\right)\\
 &=&  W^{1/2}_{\mathrm{G}}e.
\end{eqnarray}
$$
That is, the sandwich estimator with working covariance is *both* that from using working covariance with the original data, and what you'd find if you, as a pre-processing step, whitened data and model and that use identity working covariance.  But *only* the latter view produces a hat matrix that is symmetric.

# References

Salem, M., Fattah, A. A., & Rady, E. H. A. (2019). A New Heteroscedasticity Consistent Covariance Matrix Estimator and Inference Based on Robust Methods. Journal of Computational and Theoretical Nanoscience, 16(7), 2687â€“2694. https://doi.org/10.1166/jctn.2019.8222
